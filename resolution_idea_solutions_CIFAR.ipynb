{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resolution_idea_solutions_CIFAR.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Fw-gSFYIbdu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31c05d13-183f-43fb-818e-4dc7fe63242a"
      },
      "source": [
        "%matplotlib inline\n",
        "!pip install thop\n",
        "\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from thop import profile\n",
        "from torchsummary import summary\n",
        "import torchvision.datasets as dsets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: thop in /usr/local/lib/python3.6/dist-packages (0.0.31.post2005241907)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from thop) (1.7.0+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->thop) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->thop) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->thop) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->thop) (0.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9c5QX7BgEp4"
      },
      "source": [
        "# training a model first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN6ifgYqgEp4"
      },
      "source": [
        "use_cuda = True\n",
        "\n",
        "# device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rr39Ia68tUnF"
      },
      "source": [
        "\n",
        "def find_res_fixed_params(N, C1, K1, B1, K3, P1, S1, Q1, R1):\n",
        "  '''\n",
        "    Keeping the number of parameters the same\n",
        "    Approach III in the paper\n",
        "  '''\n",
        "  # N1=20 # orig image size\n",
        "\n",
        "  # # layer 1\n",
        "  # C1=3 # filter size old\n",
        "  # K1=20  # num filters old\n",
        "  # P1=0 # padding old\n",
        "  # S1=1 # stride old\n",
        "\n",
        "  # layer 2\n",
        "  # B1=2 # filter size old\n",
        "  # K3=10 # num filters old\t\n",
        "  # Q1=0 # padding old\n",
        "  # R1=1 # stride old\n",
        "\n",
        "  num_solutions = 0 \n",
        "  Solutions = []\n",
        "\n",
        "  # for C1 in np.arange(3,N1/2,2): # filter size old\n",
        "  for N2 in np.arange(N1+1,33,2): # new img resolution\n",
        "    # for B1 in np.arange(2,N2/2): # filter size old\n",
        "      for K2 in np.arange(K1,2,-1): # new number of filters in layer 1\n",
        "        for P2 in np.arange(P1,5): # new padding size in layer 1\n",
        "          for S2 in np.arange(S1,5): # new stride in layer 1\n",
        "            ratio = np.sqrt(K1/K2)\t\t\t\t\n",
        "            if np.int(ratio) != ratio: continue\n",
        "            C2 = ratio * C1\n",
        "\n",
        "            M2 = (N2 - C2 + 2*P2)  / S2  \n",
        "            M1 = (N1 - C1 + 2*P1)  / S1\n",
        "            if M2 != M1: continue\n",
        "\n",
        "            # 2nd layer\n",
        "            # for Q2 in np.arange(Q1,5): # new padding size in layer 2\n",
        "            #   for R2 in np.arange(R1,5): # new stride size in layer 2\n",
        "            for Q2 in np.arange(3): # new padding size in layer 2\n",
        "              for R2 in np.arange(1,2): # new stride size in layer 2\n",
        "\n",
        "                B2 = ratio * B1 \n",
        "                if np.int(B2) != B2: continue\t\n",
        "                if (M2 - B2 + 2*Q2)  / R2  !=  (M1 - B1 + 2*Q1)  / R1: continue\n",
        "                M3 = (M2 - B2 + 2*Q2)  / R2 \n",
        "\n",
        "                if B2 > M2/2: continue\t\t\t\t\t\t\t\t\n",
        "\n",
        "                # print(f'found a solution {C1}, {B1}, {N2}, {K2}, {P2}, {S1}, {B2}, {Q2}, {R2}')\n",
        "\n",
        "                # print(f'1st layer old: inp res: {N1}, filter size: {C1}, num filters: {K1}, out res: {M1}, stride: {S1}, pad: {P1}')\n",
        "                # print(f'1st layer new: inp res: {N2}, filter size: {C2}, num filters: {K2}, out res: {M2}, stride: {S2}, pad: {P2}')\n",
        "                # print(f'2nd layer old: inp res: {M1}, filter size: {B1}, out res: {M3}, stride: {R1}, pad: {Q1}')\n",
        "                # print(f'2nd layer new: inp res: {M1}, filter size: {B2}, out res: {M3}, stride: {R2}, pad: {Q2}\\n')\n",
        "                params_old = (K1 * C1**2 * 3) + (K3 * B1**2 * K1) \n",
        "                params_new = (K2 * C2**2 * 3) + (K3 * B2**2 * K2) \n",
        "\n",
        "                assert params_old==params_new, False\n",
        "                # print(f'PARAMs old: {params_old}, PARAMs new: {params_new}\\n')\n",
        "\n",
        "                num_solutions += 1\n",
        "                Solutions.append({'im_res_orig':N1, 'im_res_new':N2, 'layer1_old':[C1, K1, S1, P1], 'layer2_old':[B1, K3, R1, Q1], 'layer1':[C2, K2, S2, P2], 'layer2':[B2, K3, R2, Q2]})\n",
        "\n",
        "  # print(f'num solutions: {num_solutions}')\n",
        "  return Solutions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcEaJXHJtcNJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDttasGMlZ02"
      },
      "source": [
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing.image import array_to_img\n",
        "\n",
        "\n",
        "class NetTest(nn.Module):\n",
        "    def __init__(self, im_res=32, conv1_size=5, conv2_size=5, pool1_size=2, pool2_size=2, num_conv1=10, num_conv2=10, stride_1=1, stride_2=1, padd_1=0, padd_2=0, num_hid=50):\n",
        "        super(NetTest, self).__init__()\n",
        "        # import pdb; pdb.set_trace()\n",
        "        self.conv1 = nn.Conv2d(3, num_conv1, kernel_size=conv1_size, stride=stride_1, padding=padd_1, bias=False)\n",
        "        self.res1 = int((im_res - conv1_size + 2*padd_1) / stride_1 + 1)\n",
        "        self.conv2 = nn.Conv2d(num_conv1, num_conv2, kernel_size=conv2_size, stride=stride_2, padding=padd_2, bias=False)\n",
        "        self.res2_pool = int((self.res1 - pool1_size + 2*0) / pool1_size + 1)                \n",
        "        self.res2_conv = int((self.res2_pool - conv2_size + 2*padd_2) / stride_2 + 1 )       \n",
        "        self.res2_pool = int((self.res2_conv - pool2_size + 2*0) / pool2_size + 1)               \n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "\n",
        "        self.fc1 = nn.Linear((self.res2_pool**2)*num_conv2, num_hid, bias=False)        \n",
        "        self.fc2 = nn.Linear(num_hid, 10, bias=False)\n",
        "        self.pool1_size = pool1_size\n",
        "        self.pool2_size = pool2_size\n",
        "        self.num_conv2 = num_conv2\n",
        "        self.im_res = im_res\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # import pdb; pdb.set_trace()\n",
        "        # x = F.avg_pool2d(x, kernel_size=28//self.im_res)\n",
        "        # x = F.avg_pool2d(x, kernel_size=2, padding=6, stride=2)\n",
        "        # x = cv2.resize(x, (self.im_res, self.im_res)) \n",
        "\n",
        "        # xx = torch.stack((x[0,0],x[0,0], x[0,0]), axis=2)\n",
        "        # img_pil = array_to_img(xx.cpu())\n",
        "        # img_pil = img_pil.resize((self.im_res,self.im_res))\n",
        "        # x = torch.tensor(img_to_array(img_pil)).cuda()\n",
        "        # x = x[:,:,0][None,None] #.permute(2,0,1)\n",
        "\n",
        "        tmp1 = F.relu(F.max_pool2d(self.conv1(x), self.pool1_size))\n",
        "        tmp = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(tmp1)), self.pool2_size))\n",
        "        # x = F.avg_pool2d(tmp, kernel_size=tmp.shape[-1])\n",
        "        # x = x[:,:,0,0]\n",
        "\n",
        "        x = tmp.view(-1, (self.res2_pool**2)*self.num_conv2)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return x   ### SUPER IMPORTANT ***************************** works with cross entropy loss loss now\n",
        "\n",
        "device = 'cuda'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NINTmFMRDpeM"
      },
      "source": [
        "# c = 6\n",
        "# p = 3\n",
        "# s = 2\n",
        "# assert 20 == (28 - c + 2*p) / s + 1, False\n",
        "# 19 = 22 + 2*6 \n",
        "# import cv2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CGZ2JLFzdgA"
      },
      "source": [
        "def train_model(model):\n",
        "\n",
        "  EPOCHS = 10\n",
        "  losses = []\n",
        "\n",
        "  optimizer = optim.Adadelta(model.parameters())\n",
        "\n",
        "  model.train()\n",
        "  for epoch in range(EPOCHS):\n",
        "      for batch_idx, (data, target) in enumerate(train_loader):\n",
        "          \n",
        "          data, target = data.to(device), target.to(device)        \n",
        "          optimizer.zero_grad()\n",
        "          y_pred = model(data) \n",
        "\n",
        "          # import pdb; pdb.set_trace()\n",
        "          loss = F.cross_entropy(y_pred, target)\n",
        "          losses.append(loss.cpu().data)\n",
        "\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          \n",
        "          # Display\n",
        "          if batch_idx % 100 == 1:\n",
        "              print('\\r Train Epoch: {}/{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                  epoch+1,\n",
        "                  EPOCHS,\n",
        "                  batch_idx * len(data), \n",
        "                  len(train_loader.dataset),\n",
        "                  100. * batch_idx / len(train_loader), \n",
        "                  loss.cpu().data), \n",
        "                  end='')\n",
        "\n",
        "  # Eval\n",
        "  # model.eval()\n",
        "  # Eval\n",
        "  model.eval()\n",
        "  correct = total = 0\n",
        "  for idx, (images, labels) in enumerate(test_loader):\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "      outputs = model(images)\n",
        "      _, pre = torch.max(outputs.data, 1)\n",
        "\n",
        "      total += len(labels)\n",
        "      correct += (pre == labels).sum()\n",
        "\n",
        "  accuracy = float(correct) / total\n",
        "  print('Test Accuracy: %f %%' % accuracy)\n",
        "  # return acc\n",
        "\n",
        "\n",
        "  # import pdb; pdb.set_trace()\n",
        "  # evaluate_x = test_loader.dataset.test_data.type_as(torch.FloatTensor()).to(device)\n",
        "  # evaluate_y = test_loader.dataset.test_labels.to(device)\n",
        "\n",
        "  # output = model(evaluate_x[:,None,...])\n",
        "  # pred = output.data.max(1)[1]\n",
        "  # d = pred.eq(evaluate_y.data).cpu()\n",
        "  # accuracy = d.sum().type(dtype=torch.float64)/d.size()[0]\n",
        "  \n",
        "  print('\\r Train Epoch: {}/{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Test Accuracy: {:.4f}%'.format(\n",
        "      epoch+1,\n",
        "      EPOCHS,\n",
        "      len(train_loader.dataset), \n",
        "      len(train_loader.dataset),\n",
        "      100. * batch_idx / len(train_loader), \n",
        "      loss.cpu().data,\n",
        "      accuracy*100,\n",
        "      end=''))\n",
        "  \n",
        "\n",
        "  return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdlukGiltyP5"
      },
      "source": [
        "# sol_3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Saj7rxrhL6Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIedXhFbNK84"
      },
      "source": [
        "def evaluate_orig_model(sol):\n",
        "\n",
        "  # run the original model for a couple of times\n",
        "  # sol = sol_3[0]\n",
        "  sol['orig_solutions'] = []\n",
        "\n",
        "  im_res = sol['im_res_orig']\n",
        "  cv1_size = int(sol['layer1_old'][0])\n",
        "  cv2_size = int(sol['layer2_old'][0])\n",
        "  no_conv1 = sol['layer1_old'][1]\n",
        "  no_conv2 = sol['layer2_old'][1]\n",
        "  st1 = sol['layer1_old'][2]\n",
        "  st2 = sol['layer2_old'][2]\n",
        "  pd1 = sol['layer1_old'][3]\n",
        "  pd2 = sol['layer2_old'][3]\n",
        "\n",
        "  global train_loader\n",
        "  global test_loader\n",
        "  \n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "      datasets.CIFAR10('../data', train=True, download=True, transform=transforms.Compose([\n",
        "              transforms.ToTensor(),transforms.Resize((im_res, im_res)),])), batch_size=100, shuffle=True)\n",
        "\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "      datasets.CIFAR10('../data', train=False, download=True, transform=transforms.Compose([\n",
        "              transforms.ToTensor(),transforms.Resize((im_res, im_res)),])), batch_size=100, shuffle=True)\n",
        "\n",
        "\n",
        "  # classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "  # num_cls = len(classes)\n",
        "  # p = next(iter(train_loader))[0].shape[-1]\n",
        "\n",
        "  try:\n",
        "    # print(f'----  im_res={im_res_new}, conv1_size={cv1_size}, conv2_size={cv2_size}')\n",
        "    # import pdb; pdb.set_trace()\n",
        "    accs = []\n",
        "    for i in range(3):\n",
        "      model = NetTest(im_res=im_res, conv1_size=cv1_size, conv2_size=cv2_size, num_conv1=no_conv1, num_conv2=no_conv2, padd_1=pd1, padd_2=pd2, stride_1=st1, stride_2=st2).to(device)\n",
        "      if i==0:\n",
        "        macs, params = profile(model, inputs=(torch.randn(1, 3, im_res, im_res).to(device), ))\n",
        "      accs.append(train_model(model))\n",
        "    \n",
        "    sol['orig_solutions'].append((macs, params, np.mean(accs)))\n",
        "    # accuracies.append((sol, macs, params, acc))\n",
        "    # print(accuracies[-1])\n",
        "    print(sol, '\\n')\n",
        "\n",
        "  except:\n",
        "    print('\\n Model invalid')\n",
        "\n",
        "\n",
        "  return #sol\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chf2kB7wVinf"
      },
      "source": [
        "def evaluate_new_model(sols):\n",
        "  # running the solutions; one by one; three each\n",
        "  # for run_no\n",
        "  for sol in sols:\n",
        "        im_res = sol['im_res_new']\n",
        "        cv1_size = int(sol['layer1'][0])\n",
        "        cv2_size = int(sol['layer2'][0])\n",
        "        no_conv1 = sol['layer1'][1]\n",
        "        no_conv2 = sol['layer2'][1]\n",
        "        st1 = sol['layer1'][2]\n",
        "        st2 = sol['layer2'][2]\n",
        "        pd1 = sol['layer1'][3]\n",
        "        pd2 = sol['layer2'][3]\n",
        "\n",
        "        sol['new_solutions'] = []\n",
        "\n",
        "        global train_loader\n",
        "        global test_loader\n",
        "\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            datasets.CIFAR10('../data', train=True, download=True, transform=transforms.Compose([\n",
        "                    transforms.ToTensor(),transforms.Resize((im_res, im_res)),])), batch_size=100, shuffle=True)\n",
        "\n",
        "        test_loader = torch.utils.data.DataLoader(\n",
        "            datasets.CIFAR10('../data', train=False, download=True, transform=transforms.Compose([\n",
        "                    transforms.ToTensor(),transforms.Resize((im_res, im_res)),])), batch_size=100, shuffle=True)\n",
        "\n",
        "\n",
        "        try:\n",
        "          # print(f'----  im_res={im_res_new}, conv1_size={cv1_size}, conv2_size={cv2_size}')\n",
        "          # import pdb; pdb.set_trace()\n",
        "          accs = []\n",
        "          for i in range(3):\n",
        "            model = NetTest(im_res=im_res, conv1_size=cv1_size, conv2_size=cv2_size, num_conv1=no_conv1, num_conv2=no_conv2, padd_1=pd1, padd_2=pd2, stride_1=st1, stride_2=st2).to(device)\n",
        "            if i==0:\n",
        "              macs, params = profile(model, inputs=(torch.randn(1, 3, im_res, im_res).to(device), ))\n",
        "            accs.append(train_model(model))\n",
        "          \n",
        "          sol['new_solutions'].append((macs, params, np.mean(accs)))\n",
        "          # accuracies.append((sol, macs, params, acc))\n",
        "          # print(accuracies[-1])\n",
        "          print(sol, '\\n')\n",
        "\n",
        "        except:\n",
        "          print('\\n Model invalid')\n",
        "\n",
        "\n",
        "        return #sol\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXpPn7xn2Mfv",
        "outputId": "bf8d2e76-ba57-43ed-eb60-2cb54b8a2e47"
      },
      "source": [
        "\n",
        "import random \n",
        "all_res = {}\n",
        "\n",
        "for N1 in [12]:#12,16,20,24]:\n",
        "  all_solutions = []\n",
        "  for C1 in np.arange(3,8,2): # filter size layer 1 \n",
        "    for K1 in np.arange(10,21,5): # num filters layer 1   \n",
        "      for B1 in np.arange(3,8,2): # filter size layer 2 \n",
        "        for K3 in np.arange(10,21,5): # num filters layer 2    \n",
        "          for P1 in np.arange(2): # padding layer 1\n",
        "            for S1 in np.arange(1,2): # stride layer 1    \n",
        "              for Q1 in np.arange(2): # padding layer 2\n",
        "                for R1 in np.arange(1,2): # stride layer 2   \n",
        "                  xx = find_res_fixed_params(N1, C1, K1, B1, K3, P1, S1, Q1, R1)\n",
        "                  if xx:\n",
        "                    all_solutions.append(xx)\n",
        "  \n",
        "  results = []\n",
        "  random.shuffle(all_solutions)\n",
        "  all_solutions.sort(key=len, reverse=True)\n",
        "  all_solutions = all_solutions[:10]\n",
        "  for idx, sol_3 in enumerate(all_solutions):\n",
        "    print(f'\\n --------------------- solution no {idx} -------------------- \\n')\n",
        "    evaluate_orig_model(sol_3[0])\n",
        "    if not sol_3[0]['orig_solutions']:\n",
        "      break\n",
        "\n",
        "    if len(sol_3) > 4:\n",
        "      sol_3 = sol_3[:4]\n",
        "\n",
        "    evaluate_new_model(sol_3)\n",
        "    xx = (sol_3[0]['orig_solutions'][0], [sol['new_solutions'][0][-1] for sol in sol_3 if sol['new_solutions']])\n",
        "\n",
        "    results.append(xx)\n",
        "\n",
        "  all_res[N1] = results          \n",
        "print(len(all_solutions))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " --------------------- solution no 0 -------------------- \n",
            "\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.dropout.Dropout2d'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class '__main__.NetTest'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            " Train Epoch: 10/10 [40100/50000 (80%)]\tLoss: 1.608110Test Accuracy: 0.511700 %\n",
            " Train Epoch: 10/10 [50000/50000 (100%)]\tLoss: 1.575675\t Test Accuracy: 51.1700%\n",
            "{'im_res_orig': 16, 'im_res_new': 23, 'layer1_old': [5, 30, 1, 1], 'layer2_old': [5, 15, 1, 2], 'layer1': [5.0, 30, 2, 4], 'layer2': [5.0, 15, 1, 2], 'orig_solutions': [(999500.0, 20750.0, 0.5117)]} \n",
            "\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.dropout.Dropout2d'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class '__main__.NetTest'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            " Train Epoch: 10/10 [40100/50000 (80%)]\tLoss: 1.475113Test Accuracy: 0.517900 %\n",
            " Train Epoch: 10/10 [50000/50000 (100%)]\tLoss: 1.621573\t Test Accuracy: 51.7900%\n",
            "{'im_res_orig': 16, 'im_res_new': 23, 'layer1_old': [5, 30, 1, 1], 'layer2_old': [5, 15, 1, 2], 'layer1': [5.0, 30, 2, 4], 'layer2': [5.0, 15, 1, 2], 'orig_solutions': [(999500.0, 20750.0, 0.5117)], 'new_solutions': [(999500.0, 20750.0, 0.5179)]} \n",
            "\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.dropout.Dropout2d'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class '__main__.NetTest'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            " Train Epoch: 10/10 [40100/50000 (80%)]\tLoss: 1.480775Test Accuracy: 0.497700 %\n",
            " Train Epoch: 10/10 [50000/50000 (100%)]\tLoss: 1.497016\t Test Accuracy: 49.7700%\n",
            "{'im_res_orig': 16, 'im_res_new': 25, 'layer1_old': [5, 30, 1, 1], 'layer2_old': [5, 15, 1, 2], 'layer1': [5.0, 30, 2, 3], 'layer2': [5.0, 15, 1, 2], 'new_solutions': [(999500.0, 20750.0, 0.4977)]} \n",
            "\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.dropout.Dropout2d'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class '__main__.NetTest'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            " Train Epoch: 10/10 [40100/50000 (80%)]\tLoss: 1.364651Test Accuracy: 0.507100 %\n",
            " Train Epoch: 10/10 [50000/50000 (100%)]\tLoss: 1.440716\t Test Accuracy: 50.7100%\n",
            "{'im_res_orig': 16, 'im_res_new': 27, 'layer1_old': [5, 30, 1, 1], 'layer2_old': [5, 15, 1, 2], 'layer1': [5.0, 30, 2, 2], 'layer2': [5.0, 15, 1, 2], 'new_solutions': [(999500.0, 20750.0, 0.5071)]} \n",
            "\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.dropout.Dropout2d'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class '__main__.NetTest'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            " Train Epoch: 10/10 [40100/50000 (80%)]\tLoss: 1.532161Test Accuracy: 0.520700 %\n",
            " Train Epoch: 10/10 [50000/50000 (100%)]\tLoss: 1.454205\t Test Accuracy: 52.0700%\n",
            "{'im_res_orig': 16, 'im_res_new': 29, 'layer1_old': [5, 30, 1, 1], 'layer2_old': [5, 15, 1, 2], 'layer1': [5.0, 30, 2, 1], 'layer2': [5.0, 15, 1, 2], 'new_solutions': [(999500.0, 20750.0, 0.5207)]} \n",
            "\n",
            "\n",
            " --------------------- solution no 1 -------------------- \n",
            "\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.dropout.Dropout2d'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class '__main__.NetTest'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            " Train Epoch: 10/10 [40100/50000 (80%)]\tLoss: 1.628757Test Accuracy: 0.469800 %\n",
            " Train Epoch: 10/10 [50000/50000 (100%)]\tLoss: 1.673953\t Test Accuracy: 46.9800%\n",
            "{'im_res_orig': 16, 'im_res_new': 25, 'layer1_old': [7, 10, 1, 2], 'layer2_old': [5, 10, 1, 4], 'layer1': [7.0, 10, 2, 4], 'layer2': [5.0, 10, 1, 4], 'orig_solutions': [(603620.0, 16970.0, 0.4698)]} \n",
            "\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.dropout.Dropout2d'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class '__main__.NetTest'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            " Train Epoch: 10/10 [40100/50000 (80%)]\tLoss: 1.633104Test Accuracy: 0.488300 %\n",
            " Train Epoch: 10/10 [50000/50000 (100%)]\tLoss: 1.572237\t Test Accuracy: 48.8300%\n",
            "{'im_res_orig': 16, 'im_res_new': 25, 'layer1_old': [7, 10, 1, 2], 'layer2_old': [5, 10, 1, 4], 'layer1': [7.0, 10, 2, 4], 'layer2': [5.0, 10, 1, 4], 'orig_solutions': [(603620.0, 16970.0, 0.4698)], 'new_solutions': [(603620.0, 16970.0, 0.4883)]} \n",
            "\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.dropout.Dropout2d'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class '__main__.NetTest'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            " Train Epoch: 10/10 [40100/50000 (80%)]\tLoss: 1.734013Test Accuracy: 0.490500 %\n",
            " Train Epoch: 10/10 [50000/50000 (100%)]\tLoss: 1.463428\t Test Accuracy: 49.0500%\n",
            "{'im_res_orig': 16, 'im_res_new': 27, 'layer1_old': [7, 10, 1, 2], 'layer2_old': [5, 10, 1, 4], 'layer1': [7.0, 10, 2, 3], 'layer2': [5.0, 10, 1, 4], 'new_solutions': [(603620.0, 16970.0, 0.4905)]} \n",
            "\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.dropout.Dropout2d'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class '__main__.NetTest'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            " Train Epoch: 10/10 [40100/50000 (80%)]\tLoss: 1.544267Test Accuracy: 0.493100 %\n",
            " Train Epoch: 10/10 [50000/50000 (100%)]\tLoss: 1.670748\t Test Accuracy: 49.3100%\n",
            "{'im_res_orig': 16, 'im_res_new': 29, 'layer1_old': [7, 10, 1, 2], 'layer2_old': [5, 10, 1, 4], 'layer1': [7.0, 10, 2, 2], 'layer2': [5.0, 10, 1, 4], 'new_solutions': [(603620.0, 16970.0, 0.4931)]} \n",
            "\n",
            "\n",
            " --------------------- solution no 2 -------------------- \n",
            "\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.dropout.Dropout2d'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class '__main__.NetTest'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            " Train Epoch: 10/10 [40100/50000 (80%)]\tLoss: 1.511435Test Accuracy: 0.479100 %\n",
            " Train Epoch: 10/10 [50000/50000 (100%)]\tLoss: 1.623793\t Test Accuracy: 47.9100%\n",
            "{'im_res_orig': 16, 'im_res_new': 25, 'layer1_old': [7, 30, 1, 2], 'layer2_old': [5, 10, 1, 4], 'layer1': [7.0, 30, 2, 4], 'layer2': [5.0, 10, 1, 4], 'orig_solutions': [(1784860.0, 24910.0, 0.4791)]} \n",
            "\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.dropout.Dropout2d'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class '__main__.NetTest'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            " Train Epoch: 10/10 [40100/50000 (80%)]\tLoss: 1.448833Test Accuracy: 0.502500 %\n",
            " Train Epoch: 10/10 [50000/50000 (100%)]\tLoss: 1.598162\t Test Accuracy: 50.2500%\n",
            "{'im_res_orig': 16, 'im_res_new': 25, 'layer1_old': [7, 30, 1, 2], 'layer2_old': [5, 10, 1, 4], 'layer1': [7.0, 30, 2, 4], 'layer2': [5.0, 10, 1, 4], 'orig_solutions': [(1784860.0, 24910.0, 0.4791)], 'new_solutions': [(1784860.0, 24910.0, 0.5025)]} \n",
            "\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.dropout.Dropout2d'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class '__main__.NetTest'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            " Train Epoch: 10/10 [40100/50000 (80%)]\tLoss: 1.455741Test Accuracy: 0.511800 %\n",
            " Train Epoch: 10/10 [50000/50000 (100%)]\tLoss: 1.532767\t Test Accuracy: 51.1800%\n",
            "{'im_res_orig': 16, 'im_res_new': 27, 'layer1_old': [7, 30, 1, 2], 'layer2_old': [5, 10, 1, 4], 'layer1': [7.0, 30, 2, 3], 'layer2': [5.0, 10, 1, 4], 'new_solutions': [(1784860.0, 24910.0, 0.5118)]} \n",
            "\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.dropout.Dropout2d'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class '__main__.NetTest'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            " Train Epoch: 3/10 [30100/50000 (60%)]\tLoss: 1.813700"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zQQFZzZAnz1",
        "outputId": "5e23a340-d534-4ad3-edc2-ec3b221abe3a"
      },
      "source": [
        "lens = ([len(x) for x in all_solutions])\n",
        "lens.sort()\n",
        "lens\n",
        "np.max(lens)\n",
        "\n",
        "all_solutions[0]\n",
        "\n",
        "\n",
        "\n",
        "# all_res[N1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'im_res_new': 29,\n",
              "  'im_res_orig': 16,\n",
              "  'layer1': [7.0, 30, 2, 4],\n",
              "  'layer1_old': [7, 30, 1, 3],\n",
              "  'layer2': [3.0, 10, 1, 0],\n",
              "  'layer2_old': [3, 10, 1, 0],\n",
              "  'orig_solutions': []},\n",
              " {'im_res_new': 31,\n",
              "  'im_res_orig': 16,\n",
              "  'layer1': [7.0, 30, 2, 3],\n",
              "  'layer1_old': [7, 30, 1, 3],\n",
              "  'layer2': [3.0, 10, 1, 0],\n",
              "  'layer2_old': [3, 10, 1, 0]}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "x8U-8XrLKrgV",
        "outputId": "9685d4f8-2873-46d6-fa9d-d5c0ce2740f5"
      },
      "source": [
        "for k in all_res.keys():\n",
        "  means = np.mean(np.array([(x[-1],np.max(y)) for x,y in all_res[k]]), axis=0)\n",
        "  # stds = np.std(np.array([(x[0][-1],np.max(y)) for x,y in all_res[k] ]), axis=0)\n",
        "  plt.plot(means)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVKpqr7n6h0B",
        "outputId": "d1b0f226-c21f-465b-be0d-27888d059501"
      },
      "source": [
        "means"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.94651, 0.95756])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxHleTnpacj_"
      },
      "source": [
        "for k in all_res.keys():\n",
        "  means = np.mean(np.array([(x[0][-1],y) for x,y in all_res[k] ]), axis=0)\n",
        "  stds = np.std(np.array([(x[0][-1],y) for x,y in all_res[k] ]), axis=0)\n",
        "  plt.plot(means)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvJH19_TC3sO"
      },
      "source": [
        "sol_3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQHMz9pRjGcr"
      },
      "source": [
        "# FLOPS\n",
        "# http://www.bnikolic.co.uk/blog/python/flops/2019/10/01/pytorch-count-flops.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEym_vLOgEp5"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ahq_bgDgEp5"
      },
      "source": [
        "aa = {2:'a'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJBPTzm-kejx"
      },
      "source": [
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwpWkGRZlWsO"
      },
      "source": [
        "colors = ['blue', 'red', 'green', ]\n",
        "for idx,i in enumerate([7,14,28]):\n",
        "  plt.plot(np.log(pars[ress==i]), score[ress==i] , 'o', color = colors[idx])\n",
        "\n",
        "plt.legend(['res = 7', 'res = 14', 'res = 28'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjPLszlGlaY4"
      },
      "source": [
        "colors = ['blue', 'red', 'green', ]\n",
        "for idx,i in enumerate([7,14,28]):\n",
        "  plt.plot(np.log(flops[ress==i]), score[ress==i] , 'o', color = colors[idx])\n",
        "\n",
        "plt.legend(['res = 7', 'res = 14', 'res = 28'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9DPWCI0mq-r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FqAhvbIobLN"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YovErL0-oUEs"
      },
      "source": [
        "# ls MyDrive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kmPE3U2of2t"
      },
      "source": [
        "cd drive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N15rtEEdolUm"
      },
      "source": [
        "# cd /MyDrive/classification_images/Pairwise\n",
        "!cd MyDrive/ClassificationImages/Pairwise/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG_7x65oonJS"
      },
      "source": [
        "# from google.colab import files\n",
        "# # \n",
        "# with open('example.txt', 'w') as f:\n",
        "#   f.write('some content')\n",
        "\n",
        "# files.download('example.txt')\n",
        "\n",
        "\n",
        "\n",
        "# with open('MyDrive/ClassificationImages/Pairwise/accs_mnist.npz', 'w') as f:\n",
        "np.savez('MyDrive/ClassificationImages/Pairwise/accs_fmnist.npz', accs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNss-tWspL-P"
      },
      "source": [
        "# with open('MyDrive/ClassificationImages/Pairwise/accs_mnist.npz') as f:\n",
        "dd = np.load('MyDrive/ClassificationImages/Pairwise/accs_mnist.npz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g07IsyKrqM7x"
      },
      "source": [
        "# accs = dd['arr_0']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIeLykpBqNQD"
      },
      "source": [
        "def test_f(aa):\n",
        "  aa.append(10)\n",
        "\n",
        "  return\n",
        "\n",
        "q = [1]\n",
        "test_f(q)\n",
        "print(q)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKYYYozCrES4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}