{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "resolution_idea_solutions_Dilated_CIFAR.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Fw-gSFYIbdu",
        "outputId": "a11beb83-7e1f-4cc5-cf09-e1ca9c6c7c48"
      },
      "source": [
        "%matplotlib inline\n",
        "!pip install thop\n",
        "\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from thop import profile\n",
        "from torchsummary import summary\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: thop in /usr/local/lib/python3.6/dist-packages (0.0.31.post2005241907)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from thop) (1.7.0+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->thop) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->thop) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->thop) (1.19.5)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->thop) (0.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9c5QX7BgEp4"
      },
      "source": [
        "# training a model first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN6ifgYqgEp4"
      },
      "source": [
        "use_cuda = True\n",
        "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
        "\n",
        "NUM_RUNS = 3\n",
        "EPOCHS = 10"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT9LC5-AoseA"
      },
      "source": [
        "# m = nn.Conv2d(1, 1, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rr39Ia68tUnF"
      },
      "source": [
        "def find_res_params_dilated(N1, C1, P1, S1):\n",
        "  '''\n",
        "    Keeping the number of parameters the same\n",
        "    Approach III in the paper\n",
        "  '''\n",
        "  # N1=20 # orig image size\n",
        "\n",
        "  # # layer 1\n",
        "  # C1=3 # filter size old\n",
        "  # K1=20  # num filters old\n",
        "  # P1=0 # padding old\n",
        "  # S1=1 # stride old\n",
        "\n",
        "  # layer 2\n",
        "  # B1=2 # filter size old\n",
        "  # K3=10 # num filters old\t\n",
        "  # Q1=0 # padding old\n",
        "  # R1=1 # stride old\n",
        "\n",
        "  num_solutions = 0 \n",
        "  Solutions = []\n",
        "\n",
        "  for N2 in np.arange(N1+1,32): # new img resolution\n",
        "    for P2 in np.arange(4): # new padding size in layer 1\n",
        "      for S2 in np.arange(1,3): # new stride in layer 1\n",
        "        for D2 in np.arange(2,4): # new dilation in layer 1      \n",
        "          # import pdb; pdb.set_trace()\n",
        "          M2 = int((N2 - D2*(C1-1) - 1 + 2*P2)  / S2  + 1)\n",
        "          M1 = int((N1 - C1 + 2*P1)  / S1 + 1)\n",
        "\n",
        "          if M2 == M1: \n",
        "            num_solutions += 1\n",
        "            Solutions.append({'im_res_orig':N1, 'im_res_new':N2, 'layer1_old':[C1, P1, S1], 'layer1':[C1, P2, S2, D2]})\n",
        "\n",
        "  # print(f'num solutions: {num_solutions}')\n",
        "  return Solutions"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDttasGMlZ02"
      },
      "source": [
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing.image import array_to_img\n",
        "\n",
        "\n",
        "class NetTest(nn.Module):\n",
        "    def __init__(self, im_res=28, conv1_size=3, conv2_size=3, pool1_size=2, pool2_size=2, num_conv1=10, num_conv2=10, stride_1=1, stride_2=1, padd_1=0, padd_2=0, num_hid=50, dilation_1=1):\n",
        "        super(NetTest, self).__init__()\n",
        "        # import pdb; pdb.set_trace()\n",
        "        self.conv1 = nn.Conv2d(3, num_conv1, kernel_size=conv1_size, stride=stride_1, padding=padd_1, dilation=dilation_1, bias=False)\n",
        "        self.res_conv1 = int((im_res - dilation_1*(conv1_size - 1) - 1 + 2*padd_1) / stride_1 + 1)\n",
        "        self.res_pool1 = int((self.res_conv1 - pool1_size + 2*0) / pool1_size + 1)                \n",
        "\n",
        "        self.conv2 = nn.Conv2d(num_conv1, num_conv2, kernel_size=conv2_size, stride=stride_2, padding=padd_2, bias=False)\n",
        "        \n",
        "        self.res_conv2 = int((self.res_pool1 - conv2_size + 2*padd_2) / stride_2 + 1 )       \n",
        "        self.res_pool2 = int((self.res_conv2 - pool2_size + 2*0) / pool2_size + 1)               \n",
        "        \n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "\n",
        "        self.fc1 = nn.Linear((self.res_pool2**2)*num_conv2, num_hid, bias=False)        \n",
        "        self.fc2 = nn.Linear(num_hid, 10, bias=False)\n",
        "        self.pool1_size = pool1_size\n",
        "        self.pool2_size = pool2_size\n",
        "        self.num_conv2 = num_conv2\n",
        "        self.im_res = im_res\n",
        "\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # import pdb; pdb.set_trace()\n",
        "\n",
        "        tmp1 = F.relu(F.max_pool2d(self.conv1(x), self.pool1_size))\n",
        "        tmp = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(tmp1)), self.pool2_size))\n",
        "        # x = F.avg_pool2d(tmp, kernel_size=tmp.shape[-1])\n",
        "        # x = x[:,:,0,0]\n",
        "\n",
        "        x = tmp.view(-1, (self.res_pool2**2)*self.num_conv2)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return x   ### SUPER IMPORTANT ***************************** works with cross entropy loss loss now\n",
        "\n",
        "device = 'cuda'\n"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NINTmFMRDpeM"
      },
      "source": [
        "# c = 6\n",
        "# p = 3\n",
        "# s = 2\n",
        "# assert 20 == (28 - c + 2*p) / s + 1, False\n",
        "# 19 = 22 + 2*6 \n",
        "# import cv2\n"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CGZ2JLFzdgA"
      },
      "source": [
        "def train_model(model):\n",
        "\n",
        "  # EPOCHS = 10\n",
        "  losses = []\n",
        "\n",
        "  optimizer = optim.Adadelta(model.parameters())\n",
        "\n",
        "  model.train()\n",
        "  for epoch in range(EPOCHS):\n",
        "      for batch_idx, (data, target) in enumerate(train_loader):\n",
        "          \n",
        "          data, target = data.to(device), target.to(device)        \n",
        "          optimizer.zero_grad()\n",
        "          y_pred = model(data) \n",
        "\n",
        "          # import pdb; pdb.set_trace()\n",
        "          loss = F.cross_entropy(y_pred, target)\n",
        "          losses.append(loss.cpu().data)\n",
        "\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          \n",
        "          # Display\n",
        "          if batch_idx % 100 == 1:\n",
        "              print('\\r Train Epoch: {}/{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                  epoch+1,\n",
        "                  EPOCHS,\n",
        "                  batch_idx * len(data), \n",
        "                  len(train_loader.dataset),\n",
        "                  100. * batch_idx / len(train_loader), \n",
        "                  loss.cpu().data), \n",
        "                  end='')\n",
        "\n",
        "  # Eval\n",
        "  # model.eval()\n",
        "  # Eval\n",
        "  model.eval()\n",
        "  correct = total = 0\n",
        "  for idx, (images, labels) in enumerate(test_loader):\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "      outputs = model(images)\n",
        "      _, pre = torch.max(outputs.data, 1)\n",
        "\n",
        "      total += len(labels)\n",
        "      correct += (pre == labels).sum()\n",
        "\n",
        "  accuracy = float(correct) / total\n",
        "  print('Test Accuracy: %f %%' % accuracy)\n",
        "  # return acc\n",
        "\n",
        "\n",
        "  # import pdb; pdb.set_trace()\n",
        "  # evaluate_x = test_loader.dataset.test_data.type_as(torch.FloatTensor()).to(device)\n",
        "  # evaluate_y = test_loader.dataset.test_labels.to(device)\n",
        "\n",
        "  # output = model(evaluate_x[:,None,...])\n",
        "  # pred = output.data.max(1)[1]\n",
        "  # d = pred.eq(evaluate_y.data).cpu()\n",
        "  # accuracy = d.sum().type(dtype=torch.float64)/d.size()[0]\n",
        "  \n",
        "  print('\\r Train Epoch: {}/{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Test Accuracy: {:.4f}%'.format(\n",
        "      epoch+1,\n",
        "      EPOCHS,\n",
        "      len(train_loader.dataset), \n",
        "      len(train_loader.dataset),\n",
        "      100. * batch_idx / len(train_loader), \n",
        "      loss.cpu().data,\n",
        "      accuracy*100,\n",
        "      end=''))\n",
        "  \n",
        "\n",
        "  return accuracy"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdlukGiltyP5"
      },
      "source": [
        "# sol_3"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Saj7rxrhL6Z"
      },
      "source": [
        ""
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIedXhFbNK84"
      },
      "source": [
        "def evaluate_orig_model(sol):\n",
        "\n",
        "  # run the original model for a couple of times\n",
        "  # sol = sol_3[0]\n",
        "  sol['orig_solutions'] = []\n",
        "\n",
        "  im_res = sol['im_res_orig']\n",
        "  cv1_size = int(sol['layer1_old'][0])\n",
        "  pd1 = sol['layer1_old'][1]\n",
        "  st1 = sol['layer1_old'][2]\n",
        "\n",
        "  global train_loader\n",
        "  global test_loader\n",
        "  \n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "      datasets.CIFAR10('../data', train=True, download=True, transform=transforms.Compose([\n",
        "              transforms.ToTensor(),transforms.Resize((im_res, im_res)),])), batch_size=100, shuffle=True)\n",
        "\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "      datasets.CIFAR10('../data', train=False, download=True, transform=transforms.Compose([\n",
        "              transforms.ToTensor(),transforms.Resize((im_res, im_res)),])), batch_size=100, shuffle=True)\n",
        "\n",
        "  # for run_no in range(1):\n",
        "  try:\n",
        "      # print(f'----  im_res={im_res_new}, conv1_size={cv1_size}, conv2_size={cv2_size}')\n",
        "      # import pdb; pdb.set_trace()\n",
        "    accs = []\n",
        "    for i in range(NUM_RUNS):\n",
        "      model = NetTest(im_res=im_res, conv1_size=cv1_size, padd_1=pd1, stride_1=st1).to(device)\n",
        "      if i==0:\n",
        "        macs, params = profile(model, inputs=(torch.randn(1, 3, im_res, im_res).to(device), ))\n",
        "      accs.append(train_model(model))\n",
        "    \n",
        "    sol['orig_solutions'].append((macs, params, np.mean(accs)))\n",
        "    # accuracies.append((sol, macs, params, acc))\n",
        "    # print(accuracies[-1])\n",
        "    print(sol, '\\n')\n",
        "\n",
        "  except:\n",
        "    print('\\n Model invalid')\n",
        "\n",
        "\n",
        "  return #sol\n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chf2kB7wVinf"
      },
      "source": [
        "def evaluate_new_model(sols):\n",
        "  # running the solutions; one by one; three each\n",
        "  # for run_no\n",
        "\n",
        "  for sol in sols:\n",
        "        im_res = sol['im_res_new']\n",
        "        cv1_size = int(sol['layer1'][0])\n",
        "        pd1 = sol['layer1'][1]\n",
        "        st1 = sol['layer1'][2]\n",
        "        dl1 = sol['layer1'][3]\n",
        "\n",
        "\n",
        "        sol['new_solutions'] = []\n",
        "\n",
        "        global train_loader\n",
        "        global test_loader\n",
        "\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            datasets.CIFAR10('../data', train=True, download=True, transform=transforms.Compose([\n",
        "                    transforms.ToTensor(),transforms.Resize((im_res, im_res)),])), batch_size=100, shuffle=True)\n",
        "\n",
        "        test_loader = torch.utils.data.DataLoader(\n",
        "            datasets.CIFAR10('../data', train=False, download=True, transform=transforms.Compose([\n",
        "                    transforms.ToTensor(),transforms.Resize((im_res, im_res)),])), batch_size=100, shuffle=True)\n",
        "\n",
        "\n",
        "        # for run_no in range(1):\n",
        "        try:\n",
        "          # print(f'----  im_res={im_res_new}, conv1_size={cv1_size}, conv2_size={cv2_size}')\n",
        "          \n",
        "          accs = []\n",
        "          for i in range(NUM_RUNS):\n",
        "            model = NetTest(im_res=im_res, conv1_size=cv1_size, padd_1=pd1, stride_1=st1, dilation_1=dl1).to(device)\n",
        "            if i==0:\n",
        "              macs, params = profile(model, inputs=(torch.randn(1, 3, im_res, im_res).to(device), ))\n",
        "            accs.append(train_model(model))\n",
        "          \n",
        "          # which_model = 'new_res' if a else 'orig_res'\n",
        "          # sol[which_model] = (macs, params, acc)\n",
        "          sol['new_solutions'].append((macs, params, np.mean(accs)))\n",
        "          # accuracies.append((sol, macs, params, acc))\n",
        "          # print(accuracies[-1])\n",
        "          print(sol, '\\n')\n",
        "\n",
        "        except:\n",
        "          print('\\n Model invalid')\n",
        "\n",
        "  \n",
        "  return #sols"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o35rJcNMqkhN"
      },
      "source": [
        "# np.arange(3,8,2)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXpPn7xn2Mfv",
        "outputId": "4083b521-b7ec-4a11-dee0-56403f58e17e"
      },
      "source": [
        "import random \n",
        "all_res = {}\n",
        "\n",
        "for N1 in [20]:#12,16,20,24]:\n",
        "  all_solutions = []\n",
        "  for C1 in np.arange(3,5,2): # filter size layer 1 \n",
        "    for P1 in np.arange(3): # padding layer 1\n",
        "      for S1 in np.arange(1,3): # stride layer 1    \n",
        "          xx = find_res_params_dilated(N1, C1, P1, S1)\n",
        "          if xx:\n",
        "            all_solutions.append(xx)\n",
        "  \n",
        "  results = []\n",
        "  random.shuffle(all_solutions)\n",
        "  all_solutions.sort(key=len, reverse=True)\n",
        "  all_solutions = all_solutions[:10]\n",
        "  for idx, sol_3 in enumerate(all_solutions):\n",
        "    print(f'\\n --------------------- solution no {idx} -------------------- \\n')\n",
        "\n",
        "    if len(sol_3) > 4:\n",
        "      sol_3 = sol_3[-4:]\n",
        "\n",
        "    evaluate_orig_model(sol_3[0])\n",
        "    if not sol_3[0]['orig_solutions']:\n",
        "      break\n",
        "\n",
        "    evaluate_new_model(sol_3)\n",
        "    xx = (sol_3[0]['orig_solutions'][0], [sol['new_solutions'][0][-1] for sol in sol_3 if sol['new_solutions']])\n",
        "\n",
        "    results.append(xx)\n",
        "\n",
        "  all_res[N1] = results          \n",
        "print(len(all_solutions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " --------------------- solution no 0 -------------------- \n",
            "\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.dropout.Dropout2d'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class '__main__.NetTest'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            " Train Epoch: 10/10 [40100/50000 (80%)]\tLoss: 1.869672Test Accuracy: 0.323200 %\n",
            " Train Epoch: 10/10 [50000/50000 (100%)]\tLoss: 1.907310\t Test Accuracy: 32.3200%\n",
            " Train Epoch: 10/10 [40100/50000 (80%)]\tLoss: 2.060081Test Accuracy: 0.255600 %\n",
            " Train Epoch: 10/10 [50000/50000 (100%)]\tLoss: 2.064050\t Test Accuracy: 25.5600%\n",
            " Train Epoch: 8/10 [100/50000 (0%)]\tLoss: 1.899703"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26uxbDiDxX6f"
      },
      "source": [
        "# all_solutions[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zQQFZzZAnz1"
      },
      "source": [
        "# ([len(x) for x in all_solutions])\n",
        "# sol_3\n",
        "# \n",
        "# len(all_solutions)\n",
        "\n",
        "\n",
        "\n",
        "all_res[N1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cPgmIM1mSSU"
      },
      "source": [
        "all_solutions[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GhWb1-mmS5a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8U-8XrLKrgV"
      },
      "source": [
        "for k in all_res.keys():\n",
        "  means = np.mean(np.array([(x[-1],np.max(y)) for x,y in all_res[k]]), axis=0)\n",
        "  # stds = np.std(np.array([(x[0][-1],np.max(y)) for x,y in all_res[k] ]), axis=0)\n",
        "  plt.plot(means)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVKpqr7n6h0B"
      },
      "source": [
        "means"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxHleTnpacj_"
      },
      "source": [
        "for k in all_res.keys():\n",
        "  means = np.mean(np.array([(x[0][-1],y) for x,y in all_res[k] ]), axis=0)\n",
        "  stds = np.std(np.array([(x[0][-1],y) for x,y in all_res[k] ]), axis=0)\n",
        "  plt.plot(means)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKYYYozCrES4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}